{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value-Based Methods\n",
    "\n",
    "#### Your task:\n",
    "- Implement and compare the following algorithms.\n",
    "\n",
    "You can use some of the environments in [https://github.com/openai/gym#environments](https://github.com/openai/gym#environments). \n",
    "\n",
    "Some of the easy environments:\n",
    "\n",
    "* `FrozenLake`\n",
    "* `CartPole`\n",
    "* `MountainCar`\n",
    "\n",
    "Note that for `CartPole` or `MountainCar` you need to discretize the state space somehow.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> GLIE Monte Carlo Control </h4>\n",
    "<p>Sample the $k$-th episode according to the policy $\\pi$ being ($\\epsilon-$greedy): \n",
    "    <ul>\n",
    "      <p><b> (Policy Evaluation) </b></p>\n",
    "     <li> Increment a counter every time that the pair $s,a$ is visited in an episode\n",
    "        $$N(s,a) \\leftarrow N(s,a)+1.$$ </li>\n",
    "    <li> Increment total return $R(s,a)\\leftarrow R(s,a)+G_t$    </li>\n",
    "    <li> Let $Q(s,a) \\sim R(s,a)/N(s,a)$ </li>\n",
    "    <li> $Q(s,a) \\leftarrow Q(s,a)$ as $N(s,a)\\rightarrow +\\infty$ </li>    \n",
    "    <p><b> (Reducing the exploration rate)</b> </p>\n",
    "    <li> $\\epsilon \\leftarrow 1/k$ </li>\n",
    "    </ul>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> Sarsa Algorithm </h4>\n",
    "Initialize $Q(s,a)$ arbitrarily. $\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}, \\ Q(\\text{terminal state},\\cdot) = 0$. \n",
    "    <ul>\n",
    "        <li> Repeat for each episode: </li>\n",
    "        Initialize the initial state $S$.\n",
    "        <p>Repeat, for each step of the episode:\n",
    "        <ul>\n",
    "            \n",
    "            <li> Choose $A$ from $S$ using the policy derived from $Q$, for instance, using $\\epsilon-$greedy</li>\n",
    "            <li> Take action $A$, observe $R$, $S'$ </li>\n",
    "            <li> Choose action $A'$ from $S'$ </li>\n",
    "            <li> $Q(S,A) <= Q(S,A)+\\alpha \\left [ R+\\gamma\\cdot Q(S',A')-Q(S,A) \\right ] $</li>\n",
    "            <li> $S <=S'$ </li>\n",
    "        </ul>\n",
    "        \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> Q-Learning Algorithm </h4>\n",
    "Initialize $Q(s,a)$ arbitrarily. $\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}, Q(\\text{terminal state},\\cdot) = 0$. \n",
    "    <ul>\n",
    "        <li> Repeat for each episode: </li>\n",
    "        Initialize the initial state $S$.\n",
    "        <p>Repeat, for each step of the episode:\n",
    "        <ul>\n",
    "            \n",
    "            <li> Choose $A$ from $S$ using the policy derived from $Q$, for instance, using $\\epsilon-$greedy.</li>\n",
    "            <li> Take action $A$, observe $R$, $S'$. </li>\n",
    "            <li> $Q(S,A) <= Q(S,A)+\\alpha \\left [ R+\\gamma\\cdot \\max_{a \\in A} Q(S',a)-Q(S,A) \\right ] $</li>\n",
    "            <li> $S <= S'$ </li>\n",
    "        </ul>\n",
    "        \n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Episode 10 finished after                 6 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 20 finished after                 6 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 30 finished after                 21 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 40 finished after                 31 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 50 finished after                 7 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 60 finished after                 8 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 70 finished after                 16 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 80 finished after                 24 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 90 finished after                 5 timesteps with r=0.0.                 Running score: 0.0\n",
      "INFO: Episode 100 finished after                 9 timesteps with r=0.0.                 Running score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "n_episodes = 100\n",
    "\n",
    "# Initialization\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "score = []    \n",
    "for j in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    policy = epsilon_greedy_policy(Q, epsilon=1./(j+1), actions = actions )\n",
    "    \n",
    "    \n",
    "    ### Generate sample episode\n",
    "    t=0\n",
    "    while not done:\n",
    "        t+=1\n",
    "        action = policy(state)    \n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "        \n",
    "        '''\n",
    "        YOUR ALGORITHM GOES HERE:\n",
    "        \n",
    "        Your policy is inherited from the value function, \n",
    "        so you need to update it here\n",
    "        ''' \n",
    "            \n",
    "        if done:\n",
    "            if len(score) < 100:\n",
    "                score.append(reward)\n",
    "            else:\n",
    "                score[j % 100] = reward\n",
    "                \n",
    "                \n",
    "            if (j+1)%10 == 0:\n",
    "                print(\"INFO: Episode {} finished after \\\n",
    "                {} timesteps with r={}.\\\n",
    "                 Running score: {}\".format(j+1, t, reward, np.mean(score)))\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
