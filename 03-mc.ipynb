{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Learning\n",
    "\n",
    "The key idea of Monte Carlo methods is to replace the explicit transition structure used by other by approximating it from average returns run over a number of simulated episodes. Here we assume that the experience gathered by the agent is dividen into episodes that eventually finish. \n",
    "\n",
    "We will look at **episodic** problems first: this means that there is a time where the game/interaction terminates, for instance, if a final state is reached. \n",
    "\n",
    "Monte Carlo Learning is model-free because we don't need to know the rewards or transitions of the underlying MDP. In particular, the estimates for one state do not \"build upon\" estimates of the other. This is useful if you only require the value at certain states. The main drawback is that it might take too much time, even for small problems. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's distinguish between **prediction** and **control** parts of our RL problem. The prediction part tells us, for a *fixed* policy, how much is that policy worth, whereas the control part tells us how to update our policy to obtain an optimal (or close to optimal).\n",
    "\n",
    "The goal is to learn $Q_\\pi$ from episodes of experience under $\\pi$:\n",
    "$$S_1, A_1, R_1, S_2, A_2, R_2, \\ldots S_T \\sim \\pi$$\n",
    "Recall that the return $G_t$ is\n",
    "$$G_t = R_{t+1}+\\gamma R_{t+2}+\\ldots + \\gamma^{T-1}R_T$$\n",
    "and that the value function is the expected return\n",
    "$$Q_\\pi(s,a) = \\mathbb E_\\pi(G_t  \\ | \\ S_t = s, A_t = a)$$\n",
    "For Monte Carlo simulation we replace the expectation above by empirical mean.\n",
    "\n",
    "To ensure that sampled average returns would converge to the value function, we need to verify that:\n",
    "- All episodes must start in a state-action pair. \n",
    "- All state-action pairs have positive probability of being selected at the start.\n",
    "\n",
    "This guarantees that for a large number of episodes, all pairs would be selected sufficently many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>First-Visit Monte Carlo Policy Evaluation </h4>\n",
    "<p>We want to evaluate a state $s$ under a fixed policy $\\pi$: \n",
    "    <ul>\n",
    "     <li> Increment a counter the first that the pair $s,a$ is visited in an episode\n",
    "        $$N(s,a) \\leftarrow N(s,a)+1.$$ </li>\n",
    "    <li> Increment total return $R(s,a)\\leftarrow R(s,a)+G_t$    </li>\n",
    "    <li> Let $Q_\\pi(s,a) \\sim R(s,a)/N(s,a)$ </li>\n",
    "    <li> $Q_\\pi(s,a) \\leftarrow Q_\\pi(s,a)$ as $N(s,a)\\rightarrow +\\infty$ </li>    \n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the counter $N(s,a)$ persist across many episodes. At this point, we only want to see how good the policy $\\pi$ is, not how to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation of this idea is the **Every-Visit Monte Carlo Policy Evaluation**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Every-Visit Monte Carlo Policy Evaluation </h4>\n",
    "<p>We want to evaluate a state $s$ under a fixed policy $\\pi$: \n",
    "    <ul>\n",
    "     <li> Increment a counter every time that the pair $s,a$ is visited in an episode\n",
    "        $$N(s,a) \\leftarrow N(s,a)+1.$$ </li>\n",
    "    <li> Increment total return $R(s,a)\\leftarrow R(s,a)+G_t$    </li>\n",
    "    <li> Let $Q_\\pi(s,a) \\sim R(s,a)/N(s,a)$ </li>\n",
    "    <li> $Q_\\pi(s,a) \\leftarrow Q_\\pi(s,a)$ as $N(s,a)\\rightarrow +\\infty$ </li>    \n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between these two? Almost none in practice, although the theoretical analysis is different: for instance, notice that the counter $N(s,a)$ is increased several times per episode. Both methods are valid estimators. They are **on-policy**, meaning that they sample and evaluate from the same policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the mean of a sequence $x_1, x_2, \\ldots$ can be computed incrementally:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu_k & = & \\frac{1}{k}\\sum_{j=1}^k x_j \\\\\n",
    " & = & \\frac{1}{k}\\left( x_k + \\sum_{j=1}^{k-1}x_j\\right) \\\\\n",
    " & = & \\frac{1}{k}\\left( x_k + (k-1)\\mu_{k-1}\\right) \\\\\n",
    " & = & \\mu_{k-1} +\\frac{1}{k}(x_k-\\mu_{k-1})\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "This means we can do incremental Monte Carlo Updates for the $Q$ function, without having to remember the full history:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Incremental Monte Carlo Algorithm</h4>\n",
    "<ul>\n",
    "     <li>For each state $S_t$ with return $G_t$ </li>\n",
    " $$\\begin{aligned}\n",
    " N(S_t,A_t) &\\leftarrow& N(S_t, A_t) +1\\\\\n",
    " Q(S_t,A_t) &\\leftarrow& Q(S_t,A_t)+\\frac{1}{N(S_t, A_t)}(G_t-Q(S_t,A_t))\n",
    " \\end{aligned}\n",
    " $$\n",
    " </ul>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"forget the past\" by compute an exponential moving mean. We don't move to correct the value all the way to the mean, we just correct it a bit.\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+\\alpha(G_t-Q(S_t,A_t))$$\n",
    "\n",
    "What's the advantage of this? By fixing the step size, we do not move all the way to the exact estimate of the mean: we do not want to remember our previous estimates of the mean payoff from far in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "\n",
    "We know how to evaluate policies. How can we do that? One idea would be to do **policy iteration**, after all, we have the value function, so we can do greedy policy iteration, as in the model based case, right?\n",
    "\n",
    "Well, there's a problem with that: following a greedy policy might not ensure that we will explore the full state-action space! So our estimates via Monte Carlo may not be correct. \n",
    "\n",
    "Instead of acting greedily, we can act rather **$\\epsilon$-greedy**: with a small probability $\\epsilon$, choose a random action. This ensures that we will visit all possible states with positive probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> GLIE Monte Carlo Control </h4>\n",
    "<p>Sample the $k$-th episode according to the policy $\\pi$: \n",
    "    <ul>\n",
    "      <p><b> (Policy Evaluation) </b></p>\n",
    "     <li> Increment a counter every time that the pair $s,a$ is visited in an episode\n",
    "        $$N(s,a) \\leftarrow N(s,a)+1.$$ </li>\n",
    "    <li> Increment total return $R(s,a)\\leftarrow R(s,a)+G_t$    </li>\n",
    "    <li> Let $Q(s,a) \\sim R(s,a)/N(s,a)$ </li>\n",
    "    <li> $Q(s,a) \\leftarrow Q_\\pi(s,a)$ as $N(s,a)\\rightarrow +\\infty$ </li>    \n",
    "    <p><b> (Policy Improvement)</b> </p>\n",
    "    <li> $\\epsilon \\leftarrow 1/k$ </li>\n",
    "    </ul>\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that GLIE is an on-policy algorithm: we are evaluating the policy we are learning from. \n",
    "\n",
    "For this particular version of GLIE, it does not matter how is $Q$ initialized. \n",
    "\n",
    "For the non-stationary version $\\left( Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+\\alpha(G_t-Q(S_t,A_t) \\right)$ the initialization of $Q$ might matter more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the implementation, we need first to implement an $\\epsilon-$greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The code below shows how to implement GLIE.\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function that runs the episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "R = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "N = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "actions = range(env.action_space.n)\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "def run_episode(env, policy): \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = new_state    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is all in place, we need to set up the training schedule: how are parameters (in this case, the $Q$ function only) going to be updated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▉                                                            | 9926/50000 [00:06<00:30, 1301.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.3845999279193392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████▍                                            | 19895/50000 [00:14<00:24, 1251.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.36370057660232397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████▏                             | 29885/50000 [00:23<00:15, 1296.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.27793598814716775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████               | 39902/50000 [00:31<00:08, 1173.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.3221021893061096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▉| 49964/50000 [00:40<00:00, 1333.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.3700988195029871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:40<00:00, 1223.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "score = 0\n",
    "n_iter = 50000\n",
    "for j in tqdm(range(n_iter)):\n",
    "    policy = epsilon_greedy_policy(Q,epsilon=100/(j+1), actions = actions )\n",
    "    episode = run_episode(env,policy)\n",
    "    ep_reward = sum(x[2]*(gamma**i) for i, x in enumerate(episode))\n",
    "    score += ep_reward # counter for the 100 episode reward\n",
    "    \n",
    "    sa_in_episode = set([(x[0],x[1]) for x in episode])\n",
    "    \n",
    "    # Find first visit of each s,a in the episode\n",
    "    for s,a in sa_in_episode:\n",
    "        first_visit = next(i for i,x in enumerate(episode) \\\n",
    "                           if x[0]==s and x[1]==a)\n",
    "        G = sum(x[2]*(gamma**i) for i, x in enumerate(episode[first_visit:]))\n",
    "        R[s,a] += G\n",
    "        N[s,a] += 1\n",
    "        Q[s,a] = R[s,a]/N[s,a]\n",
    "\n",
    "    if (j+1)%10000 == 0: print(\"Score: \", score/100, end=\"\")\n",
    "    \n",
    "    if j%100 == 0:\n",
    "        score = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
