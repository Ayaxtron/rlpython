{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks and Experience Replay\n",
    "\n",
    "Gradient methods are intuitively appealing: you just move a little bit in the downhill direction. They have a serious drawback, however: they are not sample-efficient. To see this, observe that we update our function approximation in the direction of the experience, and then throw the experience away.\tIt would make more sense to find the best fitting value function given the agent's experience. We would be better off if we process batches of this experience, instead of throwing it all away.\n",
    "\n",
    "An **experience** or **replay memory** $\\mathcal D$ is a collection of tuples $(s,a,r)$. We will train the agent chooses randomly a minibatch from $\\mathcal D$, to replay his experience and update $\\theta$. Why choosing randomly? This helps break correlations from the data. For example, if you spent the second half of your time in an episode doing something completely useless from the reward point of view, you would not learn much if you take, say, the last 10 moves in your batch. This idea was introduced in a [*Nature* paper in 2015](https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html), and has really impacted much of the research in the last two years, at least in what concerns video game playing. The DQN algorithm works as follows:\n",
    "\n",
    "* Take action $a$ according to $\\epsilon$-greedy policy. \n",
    "* Store transition $s,a,r,s'$ in replay memory $\\mathcal D$\n",
    "* Choose a random sample from $\\mathcal D$ (minibatch).\n",
    "* Compute $Q$-learning target with old, fixed parameters $w^-$.\n",
    "* Choose the new parameter $w$ that minimizes the error\n",
    "$$ \\sum_{s,a,r,s'}\\left ( r+ \\gamma \\max_{a'} Q(s',a',w^-)-Q(s,a,w)\\right )^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Atari corpus consist of a number of Atari games. For each of them the environment works as follows:\n",
    "- Input: stack of raw pixels from last 4 frames.\n",
    "- Reward is change of score for the step.\n",
    "- Output: 18 joystick/button positions.\n",
    "\n",
    "The training time reported with the previous architecture is 2 weeks on GPU to reach human-level performance. A remarkable fact is that the same architecture was used for all the games.\n",
    "\n",
    "\n",
    "Two tricks that make DQN work (recall the non-convergence discussion from last lecture):\n",
    "- Experience Replay: because of the correlation-breaking feature discussed above.\n",
    "- Fixed Q-target: For a while, we are improving on the direction of the frozen parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements since the original DQN\n",
    "\n",
    "\n",
    "\n",
    "### Double DQN\n",
    "- **Issue**: Overestimation of the actions\n",
    "- It remains an open problem whether overestimation of the actions is an issue. \n",
    "\t- What can go wrong? Be too optimistic about bad actions.\n",
    "- Not a \"deep learning\" issue, the same happens in tabular methods (NIPS 2010).\n",
    "- Current network is used to select actions. \n",
    "- Older network is used to evaluate actions.\n",
    "- Error to minimize is:\n",
    "\n",
    "$$ \\sum_{s,a,r,s'}\\left ( r+ \\gamma Q(s', \\mathrm{argmax}_{a'}Q(s',a',w),w^-)-Q(s,a,w)\\right )^2$$\n",
    "\n",
    " \n",
    "### Prioritised replay\n",
    "- State transitions can be more or less surprising, irrelevant or even not relevant for the current agent level.\n",
    "- Replay transitions with high expected learning progress\n",
    "- Store the experience in a priority queue, depending to the DQN error\n",
    "\t$$|r+\\gamma \\max_{a'}(s',a',w^-)-Q(s,a,w)|$$ \n",
    "- Some noise in the selection needed to reduce bias and loss of diversity.\n",
    "- Similar results as in the DQN paper, but faster.\n",
    "\n",
    "\n",
    "\n",
    "### Duelling network\n",
    "\n",
    "- Split the $Q$-network into two channels\n",
    "\t$$Q(s,a) = V(s,u)+ A(s,a,w).$$\n",
    "- More efficient learning, because the updates of the value function $V$ do not depend on the action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with experience replay (Linear Approximator)\n",
    "\n",
    "In the following code sample, we use the experience replay idea, although with a twist for pedagogical purposes: instead of using neural networks, we use a simple linear function approximator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "class LinearEstimator:\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        self.DIM = 5\n",
    "        self.w = 2*np.random.random(self.DIM)-1\n",
    "        self.alpha = 0.01\n",
    "        self.D = []\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        \n",
    "    def featurize(self,s,a):\n",
    "        x = np.zeros(self.DIM)\n",
    "        x[0] = s[0]\n",
    "        x[1] = s[1]\n",
    "        x[2] = s[2]\n",
    "        x[3] = s[3]\n",
    "        x[4] = -1 if a==0 else 1\n",
    "        return x\n",
    "        \n",
    "    def predict(self,s,a):\n",
    "        return np.matmul(self.featurize(s,a),self.w)\n",
    "    \n",
    "    def remember(self,s,a,r,s1):\n",
    "        self.D.append((s,a,r,s1))\n",
    "        \n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.D,self.batch_size)\n",
    "        \n",
    "        def Q(s,a,w):\n",
    "            return np.matmul(self.featurize(s,a),w)\n",
    "        \n",
    "        def target(w):\n",
    "            w_ = self.w\n",
    "            tot_error = 0\n",
    "            for b in batch:\n",
    "                s,a,r,s_ = b\n",
    "                q_max = max(Q(s,0,w_), Q(s,1,w_))\n",
    "                tot_error += (r+ q_max-Q(s_,a,w))**2\n",
    "            return tot_error\n",
    "        \n",
    "        w0 = np.random.rand(len(self.w))\n",
    "        res = minimize(target, w0)\n",
    "        return res.x\n",
    "        \n",
    "        \n",
    "        \n",
    "def epsilon_greedy_policy(estimator, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax([estimator.predict(state,a) \\\n",
    "                                for a in actions])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "estimator = LinearEstimator()\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "n_episodes = 50000\n",
    "update_freq = 100\n",
    "initial_train = 1000\n",
    "actions = range(env.action_space.n)\n",
    "# TO DO:\n",
    "score = []    \n",
    "for e in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    policy = epsilon_greedy_policy(estimator,epsilon=0.1, \n",
    "                                   actions = actions )\n",
    "    \n",
    "    step_count = 0\n",
    "    ep_reward = 0\n",
    "    \n",
    "    \n",
    "    ### Generate sample episode\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        action = policy(state)\n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        #Calculate the td_target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            new_q_val = estimator.predict(new_state,new_action)\n",
    "            td_target = reward + gamma * new_q_val\n",
    "        \n",
    "        estimator.remember(state,action,reward, new_state)    \n",
    "        \n",
    "        if step_count % update_freq == 0 and e>initial_train:\n",
    "            estimator.train_model()\n",
    "        \n",
    "        state = new_state\n",
    "            \n",
    "        if done:\n",
    "            if len(score) < 100:\n",
    "                score.append(ep_reward)\n",
    "            else:\n",
    "                score[e % 100] = ep_reward\n",
    "                #print(\"\\rEpisode {} / {}. 100 ep score:\\\n",
    "                # {}\".format(e+1, n_episodes,np.mean(score)), end=\"\")\n",
    "            # Stop training when reaching milestone\n",
    "            if np.mean(score)>195:\n",
    "                print(\"SOLVED\")\n",
    "\n",
    "            break\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Beat the benchmark! But this time with a different function approximator. Linear functions won't cut it in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
