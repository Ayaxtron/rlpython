{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function approximation \n",
    "\n",
    "\n",
    "Up to now, we have been calculating $Q(s,a)$ by different methods:\n",
    "\t \n",
    "The core idea has been always the same: Store in a lookup table the statistics of how good an action is on every state.\t\t\n",
    "\n",
    "\n",
    "What could possibly go wrong here? Well, it turns out this is not really useful to solve really **large** problems:\n",
    "\n",
    "- Backgammon: $10^{20}$ states.\n",
    "- Go: $10^{170}$ states.\n",
    "- Chess: $10^{120}$ states.\n",
    "- Helicopter flying: continuous state space.\n",
    "- Atoms in the observable universe: $10^{81}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way out of this is through **generalization**: how can we learn about the rest of the world from limited experience? To do generalization in our context, we will approximate the $Q$ function by a parameterized version. The goal is then to find a parameter vector $\\theta$, living in a suitable parameter space, such that\n",
    "\n",
    "$$\\hat{Q}(s,a,\\theta) \\approx Q(s,a)$$\n",
    "\n",
    "Our parameter $\\theta$ might be, for instance the weights in a neural network, or the coefficients for a linear regression model.\n",
    "\n",
    "We focus on differentiable function approximators, as this allows us to define \"good\" search directions to look at, but in principle we could try anything else (random forests, wavelets, Fourier basis expansion).\n",
    "\n",
    "This is almost supervised learning, except that the data is **not stationary**: A modification of the policy parameter $\\theta$ would have influence on the rest of the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall first gradient descent. Let $J(\\theta)$ be a differentiable function of $\\theta$. The **gradient** of $J(\\theta)$ is defined as:\n",
    "\t$$ \\nabla_\\theta J(\\theta) = \\left( \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\ldots  , \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\right)^T$$\n",
    "\n",
    "This quantity simply represents the direction on which the function is growing. To find a local minimum, all we need to do is to keep changing $\\theta$ in the direction of the gradient, that is,$\\Delta \\theta := -\\frac{1}{2} \\alpha\\nabla_\\theta J(\\theta)$ where $\\alpha$ is a hyperparameter, called the **step size**.\n",
    "\n",
    "\n",
    "**What does it have to do with RL?** Our goal is to find a parameter $\\theta^*$ that minimizes:\n",
    "$$J(\\theta) := \\frac{1}{2}\\sum_{s \\in \\mathcal S, a \\in \\mathcal A}\\left[ (Q(s,a)-\\hat{Q}(s,a,\\theta))^2 \\right]$$\n",
    "\n",
    "where $Q(s,a)$ is the true value function and $\\hat{Q}(s, a, \\theta)$ is the approximation.\n",
    "By doing the gradient descent update:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\Delta \\theta  &=& -\\frac{1}{2} \\alpha\\nabla_\\theta J(\\theta) \\\\\n",
    "    &=& \\mathbb \\alpha \\mathbb E \\left[ (Q(s,a)-\\hat{Q}(s,a,\\theta))\\right ]\\nabla_\\theta \\hat{Q}(s,a,\\theta)\n",
    "    \\end{aligned} $$\n",
    "    \n",
    "There is, however, a drawback to this:  We still need to calculate the expectation! (pass over all states). We can solve this by issue by doing **stochastic gradient descent**, instead of the full gradient descent update. We choose a few directions and we replace the estimation of the gradient by the average across those directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the previous steps can help us find a value function approximation in the event we have discrete states and actions. What happens if the state itself is continuous? In this case we represent the full state as **features**: These could be, for instance, the distance from a robot to (each) wall.\n",
    "\n",
    "We have an embedding of the state space into a smaller dimensional space:\n",
    "\n",
    "$$ s \\mapsto \\mathbf x(s) := (\\mathbf x_1(s), \\mathbf x_2(s), \\ldots \\mathbf x_m(s)) $$\n",
    "\n",
    "and we can represent the value function as a linear combination of features:\n",
    "$$ \\hat{v}(s,\\theta) := \\sum_{i =1}^m\\mathbf x_i(s)\\theta_i $$\n",
    "\n",
    "The update becomes:\n",
    "$$ \\Delta \\theta = \\alpha (Q(s,a)-\\hat{Q}(s,a,\\theta))\\mathbf x(s)$$\n",
    "\n",
    "where $\\mathbf x(s) = (\\mathbf x_1(s), \\mathbf x_2(s), \\ldots \\mathbf x_m(s))$\n",
    "\n",
    "\n",
    "\n",
    "As a final remark, note that table lookup is a special case of value function approximation. Note also that we somewhat assume that we know the true value function. That is, of course, *cheating*: we do not know the value function, (there is no supervisor), we only have rewards. How do we stop cheating then?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of state-value functions as the following mappings:\n",
    "\n",
    "- In DP: $s \\mapsto \\mathcal R^a_s + \\gamma\\sum_{s' \\in \\mathcal S}\\mathcal P^a_{ss'}\\cdot  \\max_{a' \\in \\mathcal A}\\hat{Q}(s',a',\\theta)$\n",
    "\n",
    "- In Monte-Carlo: $s \\mapsto G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+\\ldots$\n",
    "\n",
    "- In TD: $s \\mapsto R_{t+1}+\\gamma \\hat{Q}(S_{t+1},A_{t+1}\\theta)$\n",
    "\n",
    " \n",
    "In general, we have something of the form $s \\mapsto g$, where $g$ is some target value. Up to now, we have been doing sort of trivial updates: moving the estimated value \"a bit more\" towards $g$, when doing SARSA or Q-Learning. However, viewing each backup as a *training example* we can use any **supervised learning** method to estimate the value function.\n",
    " \t\n",
    "How to stop cheating, as per the previous section? Instead of the true value function $v(s)$, or the action-value function $Q(s,a)$, we plug in the corresponding updates as in the previous slide.\n",
    "\n",
    "\n",
    "We do this by **bootstrapping**, which means, updating the value function from other estimates.  Since off-policy methods do not backup state and action pairs with the same function they are estimating, at least theoretically, it is possible that the $Q-$learning with function approximation will not converge. In practice, it often does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        self.w = np.ones((16,4))\n",
    "        self.alpha = 0.01\n",
    "        \n",
    "    def featurize(self,s):\n",
    "        x =np.zeros(16)\n",
    "        x[s] = 1\n",
    "        return x\n",
    "        \n",
    "    def predict(self,s):\n",
    "        return np.matmul(self.featurize(s),self.w)\n",
    "    \n",
    "    def update(self,s,a, td_target):\n",
    "        error = td_target-self.predict(s)[a]\n",
    "        self.w[s,a] += self.alpha*error\n",
    "        \n",
    "    \n",
    "def epsilon_greedy_policy(estimator, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(estimator.predict(state))\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "estimator = Estimator()\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "n_episodes = 10000\n",
    "\n",
    "\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "score = []    \n",
    "for j in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    policy = epsilon_greedy_policy(estimator, \\\n",
    "                                   epsilon=100./(j+1), actions = actions )\n",
    "    \n",
    "    \n",
    "    ### Generate sample episode\n",
    "    while not done:\n",
    "        \n",
    "        action = policy(state)\n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "                       \n",
    "        #Calculate the td_target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            new_q_val = estimator.predict(new_state)[new_action]\n",
    "            td_target = reward + gamma * new_q_val\n",
    "        \n",
    "        estimator.update(state,action, td_target)    \n",
    "        state, action = new_state, new_action\n",
    "            \n",
    "        if done:\n",
    "            if len(score) < 100:\n",
    "                score.append(reward)\n",
    "            else:\n",
    "                score[j % 100] = reward\n",
    "            #print(\"\\rEpisode {} / {}.\\\n",
    "            # Avg score: {}\".format(j+1, \\\n",
    "            # n_episodes, np.mean(score)), end=\"\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "- As usual, try to beat the benchmark. You can use different estimators, instead of linear functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
