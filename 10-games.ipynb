{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Two player, zero-sum games\n",
    "\n",
    "\n",
    "\n",
    "Game theory is concerned with modelling strategic decision making for more than one agent. What does it have to do with reinforcement learning? It turns out that an agent can improve by playing against himself!. This idea is not new, but has turned out to be incredibly powerful. \n",
    "\n",
    "Formally, a zero sum game is a strategic interaction between two players. We can classify games by **information**: do both players have the same information at all decision points? Another useful criteria is the way the interaction is done: are moves simultaneous or alternating? Note that this also influences the information available to each player. \n",
    "\n",
    "Examples of games include:\n",
    "\n",
    "- Chess: Perfect information, alternating and deterministic moves. \n",
    "- Rock, paper, scissors: Perfect information with simultaneous moves.\n",
    "- Poker: Incomplete information, alternating moves. \n",
    "\n",
    "\n",
    "Why have games received attention? Recently, *Libratus*, an AI, won 1.7 million in chips (no real money) in Texas Hold-em. It played for 20 days, 11 hours per day, 4 of the top players. Why is this a thing? Unlike chess or go, poker is an incomplete information game, and such games were long believed to be too hard for computers. \n",
    "\n",
    "Around a month earlier than *Libratus*, a Czech team of researchers from Charles University and Czech Technical University created a similar AI, called *DeepStack*. You can read more about the comparison between them [here](http://www.nature.com/news/how-rival-bots-battled-their-way-to-poker-supremacy-1.21580). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some game theory terminology\n",
    "\n",
    "- A player plays a **pure** strategy if he chooses a single action. \n",
    "- A player plays a **mixed** strategy is he chooses actions randomly.\n",
    "- The expected payoff is computed simply as the expectation using the random strategies.\n",
    "- A **best reply** strategy for player $A$, given the strategy of player $B$, maximizes $A$'s expected payoff.\n",
    "- A **Nash equilibrium** is a pair of strategies such that each player's strategy is a best reply to the adversary.\n",
    "- No player can improve by changing strategy alone.\n",
    "- To **solve** a game means to play well in the game (maximize payoff). \n",
    "- One *solution concept* is Nash Equilibrium, other possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prisoners' Dilemma\n",
    "\n",
    "A game theory classic: two prisoners are in jail and they are offered (each of them separately) a deal from the prosecution: confess they comitted the crime and receive a reduced sentence, or stick to their silence. If one confess and the other doesn't, the deal is more favorable for the betrayer. Sample payoffs are illustrated below:\n",
    "\n",
    "<img src=\"images/pdilemma.png\" width=600 height=400>\n",
    "In this case, the Nash equilibrium $(C,C)$ does not give the best payoff. We say that the equilibrium is *inefficient*. The outcome $(D,D)$ is called Pareto optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Battle of the sexes\n",
    "\n",
    "A couple wants to spend some time together, but they prefer to do different things: the one who gets away with his/her activity gets a better payoff. No genders made explicit, to avoid controversy :).\n",
    "\n",
    "<img src=\"images/battle.png\" width=600 height=400>\n",
    "In this case, the game has two pure equilibria, $(S,S)$, and $(H,H)$. It also has one mixed equilibria, that can be reached through a **correlation device**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are non-zero sum games. By contrast, a **two-player**, **zero-sum game** is a strategic interaction between two agents, called **players** where both try to maximize their reward. This means that one player's win is another players' loose. Of course, this is strategically equivalent to **constant-sum** games, where the sum of the payoffs for all the outcomes is constant. Examples are chess and go, but also penalty kicking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should players do on a game? There are different frameworks to think about this\n",
    "\n",
    "- Rational Expectations\n",
    "- Play an inherited strategy\n",
    "- Copy a succesful strategy\n",
    "- Adapt the strategy to the outcome\n",
    "\n",
    "There is no universally accepted framework (perhaps, not even possible to). Rational expectations is the dominating framework for economic applications, whereas playing an inherited strategy is more used in the context of **evolutionary game theory**. We will focus on adaptive strategies, but let us just illustrate rational expectations with an example. Rational expectation means behave as if everyone was rational. This is illustrated by a game called *Generalized beauty contest* that goes back to Keynes (early  20th century).\n",
    "The game is as follows:\n",
    "\n",
    "- Guess a number from zero to 100.\n",
    "- The winner is the one whose guess is as close as possible to two-thirds of the average guess of all those participating in the contest\n",
    "- **Example**: Three players guess 20, 30, 40. The winner is ?\n",
    "\n",
    "\n",
    "What happens? a zero-level thinker may think: \"Oh, God, maths... I'll just say 50\". A first-level thinker (assuming that others will behave rationally) will say:\n",
    "\n",
    "\"These guys will say 50. So $\\frac{2}{3}\\cdot$ 50 $\\approx$ 33\". \n",
    "\n",
    "Yet a 2nd-level thinker might try to outsmart them and think:\n",
    "\n",
    "\"These guys are smart, and think the others aren't. So 22.\" \n",
    "\n",
    "What happens? Eventually, the equilibrium point reached by a hyper-rational player would then conclude that the prize should be zero.\n",
    "\n",
    "Ok, so this is what theory predicts, but what do people do? [This chart](https://www.ft.com/content/6149527a-25b8-11e5-bd83-71cb60e8f08c) from the Financial Times might give you a clue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive strategy\n",
    "\n",
    "\n",
    "\n",
    "The goal of adaptive strategies is to get Nash Equilibrium strategy from simple rules. By definition, we know that, on expectation, the NE strategy will not do worse than a tie. Since most interesting games have a certain component of luck, there is no guarantee to get a super-unbeatable strategy (and this holds for **any** strategy, not only NE). A NE strategy will then just aim to play perfect defence.\n",
    "\n",
    "The way players can reach the Nash equilibrium strategy is through **self-play**. This roughly goes as follows:\n",
    "- Two agents start with a random strategy.\n",
    "- They improve their strategy after each game by some heuristic.\n",
    "- When convergence is reached, your strategy is ready to go.\n",
    "\n",
    "\n",
    "### Fictitious Play\n",
    "\n",
    "Our first algorithm is called fictitious play, and goes back to the 50's.\n",
    "\n",
    "Starting from $(i_1,j_1) \\in I \\times J$, consider:\n",
    "$$ x_n := \\frac{1}{n}\\sum_{t=1}^n i_t, \\  \\  y_n := \\frac{1}{n}\\sum_{t=1}^n j_t.$$\n",
    "\n",
    "A sequence $(i_n,j_n)_{n\\geq 1}$ is a realization of a **fictitious play** procedure if:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "i_{n+1} \\in BR_1(y_n) \\\\\n",
    "j_{n+1} \\in BR_2(x_n) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "It is known that fictitious play converges for zero-sum games and few other special games. A DeepMind 2015 paper (http://proceedings.mlr.press/v37/heinrich15.pdf) extends FP to extensive form games. On that paper, the authors claim that:\n",
    "\n",
    "*FSP has a lot of potential to scale to large and even continuous-action game-theoretic applications* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret matching\n",
    "\n",
    "**Regret matching** was introduced by Hart and Mas-Collel, 2000. The idea is that players reach equilibrium by tracking their regrets, and will play in the future those actions that led to higher regret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "    <li>Suppose we play rock, paper, scissors for money. Each of us puts 100 EUR on the table, we play and the winner takes the money. </li>\n",
    "    <li> The strategy sets are $I=J=\\{0,1,2\\}$ and the payoff is $u(i,j),$ which takes values between $-200, 200$. </li>\n",
    "    <li> You play rock and I play paper, so I win and take 100 EUR from you. Our payoffs are $(+100,-100)$. </li>\n",
    "    <li> Your \\textbf{regret} for not playing paper is 100, but your regret for not paying scissors is even higher (200). </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Regret matching algorithm</h4>\n",
    "<ul>\n",
    "    <li> Initialize a counter for cumulative regret. </li>\n",
    "    <li> After each round, compute your regret for each action. If we played $(i^*,j^*)$, the regret for action $i$ is:\n",
    " $$\\max(u(i,j^*)-u(i^*,j^*),0).$$ </li>\n",
    "    <li> Add the regrets and normalize the sum (divide the regret of each strategy by the sum of regrets). </li>\n",
    "    <li> Play a mixed strategy that where each action is played proportionally to the cumulative regret. </li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regret matching is the basic ingredient of Libratus, the poker champion AI. It uses a selfplay algorithm (called CFR+) together with a special solver for the end of the game and a continual improvement meta-algorithm which improves after each match.\n",
    "\n",
    "CFR stands for *conterfactual regret minimization*. *Counterfactual* means \"If I had known\".This algorithm is roughly an adaptation of regret matching to the tree structure of the game.\n",
    "\n",
    "Note that for regret matching the strategy profile that converges to the Nash equilibrium is the **average** of all strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises\n",
    "- Implement fictitious play and regret matching. The user gives a matrix and your program should calculate the Nash equilibrium through self play using either algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
