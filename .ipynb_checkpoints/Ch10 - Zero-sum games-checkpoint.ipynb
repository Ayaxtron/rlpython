{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Two player, zero-sum games\n",
    "\n",
    "\n",
    "\n",
    "### Why games? \n",
    "\n",
    "- Modelling strategic decision making for more than one agent. \n",
    "- An agent can improve by playing against himself.\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Chess: Perfect information, alternating and deterministic moves. \n",
    "- Rock, paper, scissors: Perfect information with simultaneous moves.\n",
    "- Poker: Incomplete information, alternating moves. \n",
    "\n",
    "\n",
    "### Recent success in Computer Poker\n",
    "- *Libratus*, an AI, won 1.7 million in chips (no real money) in Texas Hold-em. \n",
    "- 20 days, 11 hours per day, 4 of the top players.\n",
    "- **Why is this a thing?**\n",
    "\t-  Imperfect information, unlike chess or go.\n",
    "\t-  In particular, the computer needs to learn to bluff. \n",
    "- Two research groups (CMU & CZ/Canada) came to the same benchmark (guess who made more fuss about it...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Some terminology\n",
    "- A player plays a **pure** strategy if he chooses a single action. \n",
    "- A player plays a **mixed** strategy is he chooses actions randomly.\n",
    "- The expected payoff is computed simply as the expectation using the random strategies.\n",
    "\n",
    "\n",
    "### Best reply\n",
    "- A **best reply** strategy for player $A$, given the strategy of player $B$, maximizes $A$'s expected payoff.\n",
    "- A **Nash equilibrium** is a pair of strategies such that each player's strategy is a best reply to the adversary.\n",
    "- No player can improve by changing strategy alone.\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "- Play well in the game. \n",
    "- One *solution concept* is Nash Equilibrium, other possible.\n",
    "\n",
    "\n",
    "\n",
    "### Example: Prisoner Dilemma\n",
    "![](images/pdilemma.png)\n",
    "The Nash equilibrium does not give the best payoff. We say that the equilibrium is *inefficient*. The outcome $(D,D)$ is called Pareto optimal. \n",
    "\n",
    "\n",
    "### Example: Battle of the sexes\n",
    "![](images/battle.png)\n",
    "- Two pure equilibria, $(S,S)$, and $(H,H)$. \n",
    "- One mixed equilibria, that can be reached through a **correlation device**. \n",
    "\n",
    "\n",
    "\n",
    "### Zero-sum games\n",
    "\n",
    "- A **two-player**, **zero-sum game** is a strategic interaction between two agents, called **players** where both try to maximize their reward. \n",
    "\n",
    "- One player's win is another players' loose.\n",
    "\n",
    "- Strategically equivalent to **constant-sum** games, where the sum of the payoffs for all the outcomes is constant.\n",
    "\n",
    "\n",
    "### Example\n",
    "![](images/soccer.png)\n",
    "\n",
    "\n",
    "## What should players do on a game?\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### Different frameworks to think about this\n",
    "- **Rational Expectations**\n",
    "- Play an inherited strategy\n",
    "- Copy a succesful strategy\n",
    "- **Adapt the strategy to the outcome**\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### Rational Expectations: \n",
    "- Behave as if everyone was rational \n",
    "- Generalized beauty contest\n",
    "\t- Guess a number from zero to 100.\n",
    "\t- The winner is the one whose guess is as close as possible to two-thirds of the average guess of all those participating in the contest\n",
    "\t- **Example**: Three players guess 20, 30, 40. The winner is ?\n",
    "\n",
    "\n",
    "\n",
    "- Zero-level thinker: \"Oh, God, maths... just 50\".\n",
    "- First-level thinker: \"These guys will say 50. So $\\frac{2}{3}\\cdot$ 50 $\\approx$ 33\".\n",
    "- 2nd-level: \"These guys are smart, and think the others aren't. So 22.\"\n",
    "- ...\n",
    "A hyper-rational player would then arrive to equilibrium zero.\n",
    "\n",
    "\n",
    "\n",
    "### What do people do?\n",
    "![](images/ft.png)\n",
    "https://www.ft.com/content/6149527a-25b8-11e5-bd83-71cb60e8f08c\n",
    "\n",
    "\n",
    "\n",
    "## Adaptive strategy\n",
    "\n",
    "\n",
    "\n",
    "- **Goal**: Get Nash Equilibrium strategy from simple rules.\n",
    "- On expectation, the NE strategy will not do worse than a tie.\n",
    "- Due to luck in the game, there is no guarantee (for **any** strategy, not only NE).\n",
    "- A NE strategy just plays perfect defence.\n",
    "\n",
    "\n",
    "\n",
    "### Finding Nash Equilibrium\n",
    "- Self-play\n",
    "\t- Two agents with a random strategy.\n",
    "\t- They improve their strategy using regret matching.\n",
    "\t- After each game, an improved strategy.\n",
    "\t- When convergence is reached, your strategy is ready to go.\n",
    "\n",
    "\n",
    "### Fictitious Play\n",
    "![](images/fplay.png)\n",
    "\n",
    "\n",
    "### \n",
    "- Convergence for zero-sum games and few other special games. \n",
    "- DeepMind 2015 paper (http://proceedings.mlr.press/v37/heinrich15.pdf):\n",
    "\t- Extension of FP to extensive form games.\n",
    "\t- Fictitious self-play (FSP):\n",
    "\t\t- FP + Reinforcement Learning + Supervised Learning.\n",
    "\n",
    "*FSP has a lot of potential to scale to large and even continuous-action game-theoretic applications* \n",
    "\n",
    "\n",
    "\n",
    "### Demo: Fictitious Play\n",
    "\n",
    "\n",
    "\n",
    "- **Regret matching**,  Hart and Mas-Collel, 2000.\n",
    "- Players reach equilibrium by tracking their regrets, and will play in the future those actions that led to higher regret.\n",
    "\n",
    "\n",
    "### Example\n",
    "![](images/regret.png)\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "![](images/regret_algo.png)\n",
    "\n",
    "\n",
    "\n",
    "### Demo: Regret Matching\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Example: Libratus\n",
    "- Selfplay algorithm (CFR+)\n",
    "- Endgame solver\n",
    "\t- Special solver for the end of the game.\n",
    "- Continual improvement meta-algorithm\n",
    "\t- Improvement after each match.\n",
    "\n",
    "### CFR+\n",
    "- **Counterfactual**: \"If I had known\"\n",
    "- An opponent playing with the perfect strategy needs more than a human lifetime of poker playing to have 95% statistical significance that it found the right strategy.\n",
    "- Difference with regret matching: adapt to the tree structure of the game.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9591929bc07a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Code sample: Regret minimization for rock, paper, scissors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Code sample: Regret minimization for rock, paper, scissors \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Base player that chooses a constant mixed strategy every time\n",
    "class Player:\n",
    "    def __init__(self):\n",
    "        self.my_moves = []\n",
    "        self.other_moves = []\n",
    "        \n",
    "    def move(self, strategy):\n",
    "        # Input: a vector of probability distributions for actions\n",
    "        # Output: a pure action\n",
    "        \n",
    "        r = random.uniform(0,1)\n",
    "        n_actions = len(strategy)\n",
    "        \n",
    "        a = 0\n",
    "        cumulative_proba = 0.0\n",
    "        \n",
    "        while a<n_actions-1:\n",
    "            cumulative_proba += strategy[a] \n",
    "            if r < cumulative_proba: return a\n",
    "            a +=1\n",
    "        return a\n",
    "    \n",
    "class RegretPlayer(Player):\n",
    "    def __init__(self):\n",
    "        super(RegretPlayer, self).__init__()\n",
    "        self.regret_sum = np.zeros(3)\n",
    "\n",
    "    def regret(self):\n",
    "        \n",
    "        if len(self.my_moves)>0:\n",
    "            my_action = self.my_moves[-1]\n",
    "            his_action = self.other_moves[-1]\n",
    "        else:\n",
    "            return np.zeros(3)\n",
    "        \n",
    "        # Payoffs from my perspective\n",
    "        my_payoff = np.zeros(3)\n",
    "        \n",
    "        # If we play the same, I don't get any payoff\n",
    "        my_payoff[his_action] = 0.\n",
    "                 \n",
    "        # I win when he plays scissors and I pay rock, \n",
    "        # or when I play the \"next\" (Rock = 0, Paper = 1, Scissors = 2)\n",
    "        my_payoff[0 if his_action == 2 else his_action + 1] = 1\n",
    "                \n",
    "        # I lose when I play scissors and he plays rock, \n",
    "        # or when I play the \"previous\" action         \n",
    "        my_payoff[2 if his_action == 0 else his_action -1] = -1\n",
    "                 \n",
    "        regrets = [my_payoff[a]-my_payoff[my_action] for a in range(3)]\n",
    "        regrets = np.array(regrets)\n",
    "        return regrets\n",
    "               \n",
    "    def get_regret_mixed_strategy(self):\n",
    "    \n",
    "        normalize_sum = 0.0\n",
    "        strategy = np.zeros(3)\n",
    "        regret = self.regret()                   \n",
    "        \n",
    "        for a in range(3):\n",
    "            strategy[a] = max(self.regret_sum[a],0)\n",
    "            normalize_sum += strategy[a]\n",
    "            \n",
    "        # If all regrets are positive, play randomly\n",
    "        if normalize_sum > 0:\n",
    "            strategy = strategy / normalize_sum\n",
    "        else:\n",
    "            strategy = np.ones(3)/3\n",
    "        \n",
    "        self.regret_sum += regret\n",
    "        \n",
    "        return strategy            \n",
    "\n",
    "def payoff(my_action, his_action):\n",
    "    if my_action == his_action: \n",
    "        return 0\n",
    "    if his_action == 0 and my_action==2 or my_action == his_action-1:\n",
    "        return -1\n",
    "    return 1\n",
    "        \n",
    "def run(n_rounds = 10):\n",
    "    p1 = RegretPlayer()\n",
    "    p2 = Player() \n",
    "    \n",
    "    total_p1 = 0.0\n",
    "    total_p2 = 0.0\n",
    "    \n",
    "    rounds_won_p1 = 0.\n",
    "    \n",
    "    plt.ion()\n",
    "    plt.axis([-0.1,n_rounds+0.1,-0.1,1.1])\n",
    "    print(\"*\"*100)\n",
    "    print(\"The match begins\")\n",
    "    print(\"*\"*100)\n",
    "    for n in range(1,n_rounds):\n",
    "        \n",
    "        regret_strategy_p1 = p1.get_regret_mixed_strategy()\n",
    "        \n",
    "        m1 = p1.move(regret_strategy_p1)\n",
    "        \n",
    "        m2 = p2.move([0.4,0.3,0.3])\n",
    "                \n",
    "        # Players update the info of the moves\n",
    "        p1.my_moves.append(m1)\n",
    "        p1.other_moves.append(m2)    \n",
    "            \n",
    "        total_p1 += payoff(m1,m2)\n",
    "        \n",
    "    \n",
    "        #### SHOW RESULTS\n",
    "        moves_map = {0:\"Rock\", 1:\"Paper\", 2:\"Scissors\"}\n",
    "        print('-'*50)\n",
    "        print(\"Regret:\")\n",
    "        print(p1.regret_sum)\n",
    "        print(\"Strategy:\", regret_strategy_p1)\n",
    "        print(\"My move: %s\" % moves_map[m1])\n",
    "        print(\"His move: %s\" % moves_map[m2])\n",
    "        print(\"Payoffs:\")\n",
    "        print(total_p1)\n",
    "        print(total_p2)\n",
    "                \n",
    "        rounds_won_p1 += 1 if payoff(m1,m2) >0 else 0\n",
    "\n",
    "        # Plot the moves\n",
    "        plt.title(\"Percentage of rounds won using a regret strategy\")\n",
    "        plt.scatter(n,rounds_won_p1/n, color = \"red\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(0.1)\n",
    "    \n",
    "run(n_rounds = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
