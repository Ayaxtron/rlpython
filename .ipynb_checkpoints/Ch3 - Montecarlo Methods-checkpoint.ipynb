{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "\n",
    "The key idea of Monte Carlo methods is to replace the explicit transition structure used by other by approximating it from average returns run over a number of simulated episodes. Here we assume that the experience gathered by the agent is dividen into episodes that eventually finish. Monte Carlo Learning is model-free because we don't need to know the rewards or transitions of the underlying MDP. In particular, the estimates for one state do not \"build upon\" estimates of the other. This is useful if you only require the value at certain states. The main drawback is that it might take too much time, even for small problems. \n",
    "\n",
    "\n",
    "The goal is to learn $Q_\\pi$ from episodes of experience under $\\pi$:\n",
    "$$S_1, A_1, R_1, S_2, A_2, R_2, \\ldots S_T \\sim \\pi$$\n",
    "Recall that the return $G_t$ is\n",
    "$$G_t = R_{t+1}+\\gamma R_{t+2}+\\ldots + \\gamma^{T-1}R_T$$\n",
    "and that the value function is the expected return\n",
    "$$Q_\\pi(s,a) = \\mathbb E_\\pi(G_t  \\ | \\ S_t = s, A_t = a)$$\n",
    "For Monte Carlo simulation we replace the expectation above by empirical mean.\n",
    "\n",
    "To ensure that sampled average returns would converge to the value function, we need to verify that:\n",
    "\t* All episodes must start in a state-action pair. \n",
    "\t* All state-action pairs have positive probability of being selected at the start.\n",
    "This guarantees that in the limit of an infinite number of episodes, all pairs would be selected infinitely many times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that MC with exploring starts has not been proven to converge! However, convergence is proven for a variation of this idea, called **First-Visit Monte Carlo Policy Evaluation**. The algorithm goes as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>First-Visit Monte Carlo Policy Evaluation </h4>\n",
    "<p>We want to evaluate a state $s$ under a fixed policy $\\pi$: \n",
    "    <ul>\n",
    "     <li> Increment a counter the first that the pair $s,a$ is visited in an episode\n",
    "        $$N(s,a) \\leftarrow N(s,a)+1.$$ </li>\n",
    "    <li> Increment total return $R(s,a)\\leftarrow R(s,a)+G_t$    </li>\n",
    "    <li> Let $Q(s,a) \\sim R(s,a)/N(s,a)$ </li>\n",
    "    <li> $Q(s,a) \\rightarrow Q_\\pi(s,a)$ as $N(s,a)\\rightarrow +\\infty$ </li>    \n",
    "    <li> $\\pi \\rightarrow \\epsilon-\\text{greedy}(\\pi)$ </li>\n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation of this idea is the **Every-Visit Monte Carlo Policy Evaluation**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>First-Visit Monte Carlo Policy Evaluation </h4>\n",
    "<p>We want to evaluate a state $s$ under a fixed policy $\\pi$: \n",
    "    <ul>\n",
    "     <li> Increment a counter every time that the pair $s,a$ is visited in an episode\n",
    "        $$N(s,a) \\leftarrow N(s,a)+1.$$ </li>\n",
    "    <li> Increment total return $R(s,a)\\leftarrow R(s,a)+G_t$    </li>\n",
    "    <li> Let $Q(s,a) \\sim R(s,a)/N(s,a)$ </li>\n",
    "    <li> $Q(s,a) \\rightarrow Q_\\pi(s,a)$ as $N(s,a)\\rightarrow +\\infty$ </li>    \n",
    "    <li> $\\pi \\rightarrow \\epsilon-\\text{greedy}(\\pi)$ </li>\n",
    "    </ul>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between these two? Almost none in practice, although the theoretical analysis is different. Both methods are **on-policy**, meaning that they sample and evaluate from the same policy.\n",
    "\n",
    "\n",
    "### GLIE Monte Carlo control\n",
    "\n",
    "Before discussing algorithms for solving the control following, let us make a remark:\n",
    "\n",
    "Observe that the mean of a sequence $x_1, x_2, \\ldots$ can be computed incrementally:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu_k & = & \\frac{1}{k}\\sum_{j=1}^k x_j \\\\\n",
    " & = & \\frac{1}{k}\\left( x_k + \\sum_{j=1}^{k-1}x_j\\right) \\\\\n",
    " & = & \\frac{1}{k}\\left( x_k + (k-1)\\mu_{k-1}\\right) \\\\\n",
    " & = & \\mu_{k-1} +\\frac{1}{k}(x_k-\\mu_{k-1})\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "This means we can do incremental Monte Carlo Updates for the $Q$ function:\n",
    "\n",
    "- Update $Q(s,a)$ incrementally after each episode.\n",
    "- For each state $S_t$ with return $G_t$\n",
    " $$\\begin{aligned}\n",
    " N(S_t,A_t) &\\leftarrow& N(S_t, A_t) +1\\\\\n",
    " Q(S_t,A_t) &\\leftarrow& Q(S_t,A_t)+\\frac{1}{N(S_t, A_t)}(G_t-Q(S_t,A_t))\n",
    " \\end{aligned}\n",
    " $$\n",
    "-  We can \"forget the past\" by compute an exponential moving mean. We don't move to correct the value all the way to the mean, we just correct it a bit.\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+\\alpha(G_t-Q(S_t,A_t))$$\n",
    " \n",
    "\n",
    "Now we are ready to introduce the GLIE algorithm. GLIE stands for *Greedy in the Limite with Infinite Exploration* and the algorithm goes as follows:\n",
    "\n",
    "- Sample an episode using policy $\\pi$\n",
    "- For each state $S_t$ and action $A_t$ in the episode, \n",
    "\n",
    "$$ \\begin{aligned}\n",
    "N(S_t,A_t) & \\leftarrow & N(S_t, A_t) + 1 \\\\\n",
    "Q(S_t, A_t) & \\leftarrow & Q(S_t, A_t) + \\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Improve the policy based on the new state-action-value function\n",
    "\t- $\\epsilon \\leftarrow 1/k$\n",
    "\t- $\\pi \\leftarrow \\epsilon-\\text{greedy}(Q)$\n",
    "\n",
    "\n",
    "\n",
    "You can check some implementation examples here:\n",
    "- https://gym.openai.com/evaluations/eval_TtcFIoaZQu6fGDIQICFKtw\n",
    "\n",
    "- https://gist.github.com/jpmaldonado/fbc572b3bb517ac0848687b6e987f9a0\n",
    "\n",
    "\n",
    "GLIE is an on-policy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy MC control\n",
    "\n",
    "The goal of off-policy methos is to learn for a **different** policy to the one we are using to generate the episode. Off-policy methods consider both a \n",
    "\t- **target policy $t$** policy, from which we want to learn.\n",
    "\t- **behavior policy $b$** policy, from which we generate the episode.\n",
    "\n",
    "Off-policy methods can be thought of learning by example, whereas on-policy methods are about learning by doing.\n",
    "We need to guarantee **coverage** that is, $t(a|s) > 0$ implies $b(a|s>0)$. For this we use importance sampling, which means estimation of expected values from a distribution given samples of another.\n",
    "\n",
    "\n",
    "### Weighted importance sampling\n",
    "![Off-policy MC control, from David Silver's slides.](images/mcwis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The code below shows how to implement GLIE.\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "R = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "N = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "actions = range(env.action_space.n)\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "def run_episode(env, policy): \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = new_state    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████▎                                             | 9922/50000 [00:06<00:30, 1331.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████▎                                 | 19881/50000 [00:14<00:27, 1076.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████▌                      | 29970/50000 [00:25<00:17, 1166.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████▌           | 39996/50000 [00:35<00:12, 783.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████▉| 49894/50000 [00:47<00:00, 1162.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 50000/50000 [00:47<00:00, 1043.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "score = 0\n",
    "n_iter = 50000\n",
    "for j in tqdm(range(n_iter)):\n",
    "    policy = epsilon_greedy_policy(Q,epsilon=1000/(j+1), actions = actions )\n",
    "    episode = run_episode(env,policy)\n",
    "    ep_reward = sum(x[2]*(gamma**i) for i, x in enumerate(episode))\n",
    "    score += ep_reward # counter for the 100 episode reward\n",
    "    \n",
    "    sa_in_episode = set([(x[0],x[1]) for x in episode])\n",
    "    \n",
    "    # Find first visit of each s,a in the episode\n",
    "    for s,a in sa_in_episode:\n",
    "        first_visit = next(i for i,x in enumerate(episode) if \n",
    "                           x[0]==s and x[1]==a)\n",
    "        \n",
    "        G = sum(x[2]*(gamma**i) for i, x in enumerate(episode[first_visit:]))\n",
    "        R[s,a] += G\n",
    "        N[s,a] += 1\n",
    "        Q[s,a] += 1/N[s,a]*(G-Q[s,a])\n",
    "\n",
    "    \n",
    "    \n",
    "    if (j+1)%10000 == 0:\n",
    "        print(\"Score: \", score/100)\n",
    "    \n",
    "    if j%100 == 0:\n",
    "        score = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
