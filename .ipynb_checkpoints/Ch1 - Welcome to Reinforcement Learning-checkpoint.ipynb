{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a framework for tackling **sequential decision problems**: what to do next in order to maximize a reward (which might be delayed), on a changing universe (which might react to our actions).\n",
    "\n",
    "Examples of this include:\n",
    "\n",
    "- Game playing: which actions are critical to win a game?\n",
    "- Learning in a \"small world\": what actions maximize pleasure / minimize pain?\n",
    "- Treatment of chronical diseases: how to evolve the treatment for a disease that creates resistance?\n",
    "\n",
    "The unifying theme on the problems above can be abstracted as follows: \n",
    "- An **agent** receives a signal from the environment, selected by Nature. \n",
    "- The agent executes an **action**. \n",
    "- Given the agents' action, Nature assigns a reward and draws a new state, which is announced to the agent. \n",
    "- The situation repeats until a terminal criterion is reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: OpenAI Gym\n",
    "\n",
    "We'll use the OpenAI Gym environment for this. (https://github.com/openai/gym). \n",
    "\n",
    "It consists of a number of Atari environments that we can use for experimenting during this course. If you haven't please install the library OpenAI gym (pip install gym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gym\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **S** is the initial state, and your aim is to reach **G**, without falling into the holes, **H**. The squares marked with **F** are frozen, which means you can step on them.\n",
    " \n",
    " \n",
    "**Note:** The environment is non-deterministic, you can slip in the ice and end up in a different state.\n",
    "\n",
    "How to use the environment?\n",
    " \n",
    "- **reset()** returns the initial state / first observation.\n",
    "- **render()** returns the current environment state. \n",
    "- **step(a)** returns what happens after action a:\n",
    "     - *new observation*: the new state.\n",
    "     - *reward*: the reward corresponding to that action in that state.\n",
    "     - *is done*: binary flag, True if the game is over. \n",
    "     - *info*: Some auxiliary stuff, which we can ignore now.\n",
    "     \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial state:  0\n",
      " and it looks like: \n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"The initial state: \", env.reset())\n",
    "print(\" and it looks like: \")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's take an action: \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"Now let's take an action: \")\n",
    "new_state, reward, done, _ = env.step(1)\n",
    "env.render()\n",
    "\n",
    "\n",
    "idx_to_action = {\n",
    "    0:\"<\", #left\n",
    "    1:\"v\", #down\n",
    "    2:\">\", #right\n",
    "    3:\"^\" #up\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a policy\n",
    " \n",
    " \n",
    "- The environment has a 4x4 grid of states. \n",
    "- For each state there are 4 possible actions. \n",
    " \n",
    "A **policy** is a function from states to actions. It tells us what we should do on each state. In this case, any array of size 16x4 is a (deterministic) policy.\n",
    " \n",
    "We can implement policies as dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize random_policy:\n",
    "def init_random_policy():\n",
    "    random_policy  = {}\n",
    "    for state in range(n_states):\n",
    "        random_policy[state] = np.random.choice(n_actions)\n",
    "    return random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We need now to define a function to evaluate our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, policy, max_episodes=100): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "    return tot_reward/(max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for the best policy: Random search\n",
    "\n",
    "As a very first example, let's try to find our policy by pure random search: we will play for some time and keep track of the best actions we can do on each state.\n",
    "\n",
    "FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████▎                                            | 2557/9999 [01:00<02:54, 42.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.75\n",
      "Best policy:\n",
      "{0: 0, 1: 3, 2: 2, 3: 3, 4: 0, 5: 2, 6: 0, 7: 3, 8: 3, 9: 1, 10: 0, 11: 3, 12: 3, 13: 2, 14: 1, 15: 2}\n"
     ]
    }
   ],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "# Random search\n",
    "for i in range(1,10000): #tip: you can use tqdm(range(1,10000)) for a progress bar\n",
    "    policy = init_random_policy()\n",
    "    score = evaluate(env,policy,100)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "    if i%5000 == 0:\n",
    "        print(\"Best score:\", best_score)\n",
    "print(\"Best policy:\")\n",
    "print(best_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see the policy in action\n",
    "def play(env,policy, render=False):\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    while not d:\n",
    "        a = policy[s]\n",
    "        print(\"*\"*10)\n",
    "        print(\"State: \",s)\n",
    "        print(\"Action: \",idx_to_action[a])\n",
    "        s, r, d, _ = env.step(a)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if d:\n",
    "            print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a small function to print a nicer policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    lake = \"SFFFFHFHFFFHHFFG\"\n",
    "    arrows = [idx_to_action[policy[i]] \n",
    "               if lake[i] in 'SF' else '*' for i in range(n_states)]\n",
    "    for i in range(0,16,4):\n",
    "        print(''.join(arrows[i:i+4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call then use the functions above to render the optimal policy. Note that the policy might not give the optimal action: recall that there is some noise involved (you can slip on the ice) which is responsible of a non-optimal action looking like optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_policy(best_policy)\n",
    "\n",
    "#play(env,best_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a different policy\n",
    "\n",
    "Let's try some different implementation of a random policy, which will be more useful later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theta = 0.25*np.ones((n_states,n_actions))\n",
    "def random_parameter_policy(theta):\n",
    "    theta = theta/np.sum(theta, axis=1, keepdims=True) # ensure that the array is normalized\n",
    "    policy  = {}\n",
    "    probs = {}\n",
    "    for state in range(n_states):\n",
    "        probs[state] = np.array(theta[state,:])\n",
    "        policy[state] = np.random.choice(n_actions, p = probs[state])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.67\n",
      "Best policy:\n",
      "{0: 0, 1: 1, 2: 0, 3: 1, 4: 0, 5: 1, 6: 0, 7: 1, 8: 3, 9: 1, 10: 1, 11: 3, 12: 2, 13: 2, 14: 1, 15: 1}\n",
      "Best score: 0.67\n"
     ]
    }
   ],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "alpha = 1e-2\n",
    "\n",
    "# Random search\n",
    "for i in range(1,10000): \n",
    "    theta = 0.25*np.ones((n_states,n_actions))\n",
    "    policy = random_parameter_policy(theta)\n",
    "    score = evaluate(env,policy,100)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "    theta = theta + alpha*(score-best_score)*np.ones((n_states,n_actions))\n",
    "    if i%5000 == 0:\n",
    "        print(\"Best score:\", best_score)\n",
    "print(\"Best policy:\")\n",
    "print(best_policy)\n",
    "print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the advantage of this? Perhaps not much right now, but this is the first step to use more sophisticated techniques over random search. Note that we do a \"gradient update\" of sorts when we change the parameter `theta` in the direction of increase of the best score. This hints that we could use other update rules, perhaps taking the output as a more sophisticated input of the game history.\n",
    "\n",
    "Another thing to notice is that we made effectively our policy **stochastic**: at every stage the agent has the possibility of choosing randomly his action. This has the effect of smoothing out the problem: we are now solving an optimization problem on a continuous, instead of a discrete space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn:\n",
    "\n",
    "- Beat the hill climbing / random search benchmark! Implement a different search method for the parameters.\n",
    "- Try the `CartPole` environment. In CartPole, the state is continuous (4 different parameters), so you need to do something on the lines of the parameterized random search example. Look at http://kvfrans.com/simple-algoritms-for-solving-cartpole/ for inspiration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
