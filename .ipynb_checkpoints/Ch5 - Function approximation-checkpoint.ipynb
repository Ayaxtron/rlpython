{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function approximation \n",
    "\n",
    "\n",
    "Up to now, we have been calculating $Q(s,a)$ by different methods:\n",
    "\t \n",
    "The core idea has been always the same: Store in a lookup table the statistics of how good an action is on every state.\t\t\n",
    "\n",
    "\n",
    "What could possibly go wrong here? Well, it turns out this is not really useful to solve really **large** problems:\n",
    "- Backgammon: $10^{20}$ states.\n",
    "- Go: $10^{170}$ states.\n",
    "- Chess: $10^{120}$ states.\n",
    "- Helicopter flying: continuous state space.\n",
    "- Atoms in the observable universe: $10^{81}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way out of this is through **generalization**: how can we learn about the rest of the world from limited experience? To do generalization in our context, we will approximate the $Q$ function by a parameterized version. The goal is then to find a parameter vector $\\theta$, living in a suitable parameter space, such that\n",
    "\n",
    "$$\\hat{Q}(s,a,\\theta) \\approx Q(s,a)$$\n",
    "\n",
    "Our parameter $\\theta$ might be, for instance the weights in a neural network, or the coefficients for a linear regression model.\n",
    "\n",
    "We focus on differentiable function approximators, as this allows us to define \"good\" search directions to look at, but in principle we could try anything else (random forests, wavelets, Fourier basis expansion).\n",
    "\n",
    "This is almost supervised learning, except that the data is **not stationary**: A modification of the policy parameter $\\theta$ would have influence on the rest of the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Let us recall first gradient descent. Let $J(\\theta)$ be a differentiable function of $\\theta$. The **gradient** of $J(\\theta)$ is defined as:\n",
    "\t$$ \\nabla_\\theta J(\\theta) = \\left( \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\ldots  , \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\right)^T$$\n",
    "\n",
    "This quantity simply represents the direction on which the function is growing. To find a local minimum, all we need to do is to keep changing $\\theta$ in the direction of the gradient, that is,$\\Delta \\theta := -\\frac{1}{2} \\alpha\\nabla_\\theta J(\\theta)$ where $\\alpha$ is a hyperparameter, called the **step size**.\n",
    "\n",
    "\n",
    "Our goal is to find a parameter $\\\\theta^*$ that minimizes:\n",
    "$$J(\\theta) := \\frac{1}{2}\\sum_{s \\in \\mathcal S, a \\in \\mathcal A}\\left[ (Q(s,a)-\\hat{Q}(s,a,\\theta))^2 \\right]$$\n",
    "\n",
    "where $Q(s,a)$ is the true value function and $\\hat{Q}(s, a, \\theta)$ is the approximation.\n",
    "By doing the gradient descent update:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\Delta \\theta  &=& -\\frac{1}{2} \\alpha\\nabla_\\theta J(\\theta) \\\\\n",
    "    &=& \\mathbb \\alpha \\mathbb E \\left[ (Q(s,a)-\\hat{Q}(s,a,\\theta))\\right ]\\nabla_\\theta \\hat{Q}(s,a,\\theta)\n",
    "    \\end{aligned} $$\n",
    "There is, however, a drawback to this:  We still need to calculate the expectation! (pass over all states). We can solve this by issue by doing **stochastic gradient descent**, instead of the full gradient descent update. We choose a few directions and we replace the estimation of the gradient by the average across those directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature vectors\n",
    "\n",
    "So the previous steps can help us find a value function approximation in the event we have discrete states and actions. What happens if the state itself is continuous? In this case we represent the full state as **features**: These could be, for instance, the distance from a robot to (each) wall.\n",
    "\n",
    "We have an embedding of the state space into a smaller dimensional space:\n",
    "\n",
    "$$ s \\mapsto \\mathbf x(s) := (\\mathbf x_1(s), \\mathbf x_2(s), \\ldots \\mathbf x_m(s)) $$\n",
    "\n",
    "and we can represent the value function as a linear combination of features:\n",
    "$$ \\hat{v}(s,\\theta) := \\sum_{i =1}^m\\mathbf x_i(s)\\theta_i $$\n",
    "\n",
    "The update becomes:\n",
    "$$ \\Delta \\theta = \\alpha (Q(s,a)-\\hat{Q}(s,a,\\theta))\\mathbf x(s)$$\n",
    "\n",
    "where $\\mathbf x(s) = (\\mathbf x_1(s), \\mathbf x_2(s), \\ldots \\mathbf x_m(s))$\n",
    "\n",
    "\n",
    "\n",
    "As a final remark, note that table lookup is a special case of value function approximation. Note also that we somewhat assume that we know the true value function. That is, of course, *cheating*: we do not know the value function, (there is no supervisor), we only have rewards. How do we stop cheating then?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function backups\n",
    "\n",
    "We can think of state-value functions as the following mappings:\n",
    "* In DP: $s \\mapsto \\mathcal R^a_s + \\gamma\\sum_{s' \\in \\mathcal S}\\mathcal P^a_{ss'}\\cdot  \\max_{a' \\in \\mathcal A}\\hat{Q}(s',a',\\theta)$\n",
    "\n",
    "* In Monte-Carlo: $s \\mapsto G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+\\ldots$\n",
    "\n",
    "* In TD: $s \\mapsto R_{t+1}+\\gamma \\hat{Q}(S_{t+1},A_{t+1}\\theta)$\n",
    "\n",
    " \n",
    "In general, we have something of the form $s \\mapsto g$, where $g$ is some target value. Up to now, we have been doing sort of trivial updates: moving the estimated value \"a bit more\" towards $g$, when doing SARSA or Q-Learning. However, viewing each backup as a *training example* we can use any **supervised learning** method to estimate the value function.\n",
    " \t\n",
    "How to stop cheating, as per the previous section? Instead of the true value function $v(s)$, or the action-value function $Q(s,a)$, we plug in the corresponding updates as in the previous slide.\n",
    "\n",
    "\n",
    "We do this by **bootstrapping**, which means, updating the value function from other estimates.  Since off-policy methods do not backup state and action pairs with the same function they are estimating, at least theoretically, it is possible that the $Q-$learning with function approximation will not converge. In practice, it often does.\n",
    "\n",
    "\n",
    "We illustrate function approximation for MountainCar, in the case of a linear function approximator. \n",
    "The MountainCar environment is described as follows:\n",
    "\n",
    "* State: 2 parameters $(x,y)$\n",
    "* Action: Accelerate backward (0), Stay (1), Accelerate forward (2)\n",
    "* An episode is solved if you get -110 points over 100 consecutive trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1 / 2000. Avg score: -1.0\r",
      "Episode 2 / 2000. Avg score: -1.0\r",
      "Episode 3 / 2000. Avg score: -1.0\r",
      "Episode 4 / 2000. Avg score: -1.0\r",
      "Episode 5 / 2000. Avg score: -1.0\r",
      "Episode 6 / 2000. Avg score: -1.0\r",
      "Episode 7 / 2000. Avg score: -1.0\r",
      "Episode 8 / 2000. Avg score: -1.0\r",
      "Episode 9 / 2000. Avg score: -1.0\r",
      "Episode 10 / 2000. Avg score: -1.0\r",
      "Episode 11 / 2000. Avg score: -1.0\r",
      "Episode 12 / 2000. Avg score: -1.0\r",
      "Episode 13 / 2000. Avg score: -1.0\r",
      "Episode 14 / 2000. Avg score: -1.0\r",
      "Episode 15 / 2000. Avg score: -1.0\r",
      "Episode 16 / 2000. Avg score: -1.0\r",
      "Episode 17 / 2000. Avg score: -1.0\r",
      "Episode 18 / 2000. Avg score: -1.0\r",
      "Episode 19 / 2000. Avg score: -1.0\r",
      "Episode 20 / 2000. Avg score: -1.0\r",
      "Episode 21 / 2000. Avg score: -1.0\r",
      "Episode 22 / 2000. Avg score: -1.0\r",
      "Episode 23 / 2000. Avg score: -1.0\r",
      "Episode 24 / 2000. Avg score: -1.0\r",
      "Episode 25 / 2000. Avg score: -1.0\r",
      "Episode 26 / 2000. Avg score: -1.0\r",
      "Episode 27 / 2000. Avg score: -1.0\r",
      "Episode 28 / 2000. Avg score: -1.0\r",
      "Episode 29 / 2000. Avg score: -1.0\r",
      "Episode 30 / 2000. Avg score: -1.0\r",
      "Episode 31 / 2000. Avg score: -0.9354838709677419\r",
      "Episode 32 / 2000. Avg score: -0.9375\r",
      "Episode 33 / 2000. Avg score: -0.9393939393939394\r",
      "Episode 34 / 2000. Avg score: -0.9411764705882353\r",
      "Episode 35 / 2000. Avg score: -0.9428571428571428\r",
      "Episode 36 / 2000. Avg score: -0.9444444444444444\r",
      "Episode 37 / 2000. Avg score: -0.9459459459459459\r",
      "Episode 38 / 2000. Avg score: -0.9473684210526315\r",
      "Episode 39 / 2000. Avg score: -0.9487179487179487\r",
      "Episode 40 / 2000. Avg score: -0.95\r",
      "Episode 41 / 2000. Avg score: -0.9512195121951219\r",
      "Episode 42 / 2000. Avg score: -0.9523809523809523\r",
      "Episode 43 / 2000. Avg score: -0.9534883720930233\r",
      "Episode 44 / 2000. Avg score: -0.9545454545454546\r",
      "Episode 45 / 2000. Avg score: -0.9555555555555556\r",
      "Episode 46 / 2000. Avg score: -0.9565217391304348\r",
      "Episode 47 / 2000. Avg score: -0.9574468085106383\r",
      "Episode 48 / 2000. Avg score: -0.9583333333333334\r",
      "Episode 49 / 2000. Avg score: -0.9591836734693877\r",
      "Episode 50 / 2000. Avg score: -0.96\r",
      "Episode 51 / 2000. Avg score: -0.9607843137254902\r",
      "Episode 52 / 2000. Avg score: -0.9615384615384616\r",
      "Episode 53 / 2000. Avg score: -0.9622641509433962\r",
      "Episode 54 / 2000. Avg score: -0.9629629629629629\r",
      "Episode 55 / 2000. Avg score: -0.9636363636363636\r",
      "Episode 56 / 2000. Avg score: -0.9642857142857143\r",
      "Episode 57 / 2000. Avg score: -0.9649122807017544\r",
      "Episode 58 / 2000. Avg score: -0.9655172413793104\r",
      "Episode 59 / 2000. Avg score: -0.9661016949152542\r",
      "Episode 60 / 2000. Avg score: -0.9666666666666667\r",
      "Episode 61 / 2000. Avg score: -0.9672131147540983\r",
      "Episode 62 / 2000. Avg score: -0.967741935483871\r",
      "Episode 63 / 2000. Avg score: -0.9682539682539683\r",
      "Episode 64 / 2000. Avg score: -0.96875\r",
      "Episode 65 / 2000. Avg score: -0.9692307692307692\r",
      "Episode 66 / 2000. Avg score: -0.9696969696969697\r",
      "Episode 67 / 2000. Avg score: -0.9701492537313433\r",
      "Episode 68 / 2000. Avg score: -0.9705882352941176\r",
      "Episode 69 / 2000. Avg score: -0.9710144927536232\r",
      "Episode 70 / 2000. Avg score: -0.9714285714285714\r",
      "Episode 71 / 2000. Avg score: -0.971830985915493\r",
      "Episode 72 / 2000. Avg score: -0.9722222222222222\r",
      "Episode 73 / 2000. Avg score: -0.9726027397260274\r",
      "Episode 74 / 2000. Avg score: -0.972972972972973\r",
      "Episode 75 / 2000. Avg score: -0.9733333333333334\r",
      "Episode 76 / 2000. Avg score: -0.9736842105263158\r",
      "Episode 77 / 2000. Avg score: -0.974025974025974\r",
      "Episode 78 / 2000. Avg score: -0.9743589743589743\r",
      "Episode 79 / 2000. Avg score: -0.9746835443037974\r",
      "Episode 80 / 2000. Avg score: -0.975\r",
      "Episode 81 / 2000. Avg score: -0.9753086419753086\r",
      "Episode 82 / 2000. Avg score: -0.975609756097561\r",
      "Episode 83 / 2000. Avg score: -0.9759036144578314\r",
      "Episode 84 / 2000. Avg score: -0.9761904761904762\r",
      "Episode 85 / 2000. Avg score: -0.9764705882352941\r",
      "Episode 86 / 2000. Avg score: -0.9767441860465116\r",
      "Episode 87 / 2000. Avg score: -0.9770114942528736\r",
      "Episode 88 / 2000. Avg score: -0.9772727272727273\r",
      "Episode 89 / 2000. Avg score: -0.9775280898876404\r",
      "Episode 90 / 2000. Avg score: -0.9777777777777777\r",
      "Episode 91 / 2000. Avg score: -0.978021978021978\r",
      "Episode 92 / 2000. Avg score: -0.9782608695652174\r",
      "Episode 93 / 2000. Avg score: -0.978494623655914\r",
      "Episode 94 / 2000. Avg score: -0.9787234042553191\r",
      "Episode 95 / 2000. Avg score: -0.9789473684210527\r",
      "Episode 96 / 2000. Avg score: -0.9791666666666666\r",
      "Episode 97 / 2000. Avg score: -0.979381443298969\r",
      "Episode 98 / 2000. Avg score: -0.9795918367346939\r",
      "Episode 99 / 2000. Avg score: -0.9797979797979798\r",
      "Episode 100 / 2000. Avg score: -0.98\r",
      "Episode 101 / 2000. Avg score: -0.98\r",
      "Episode 102 / 2000. Avg score: -0.98"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\.conda\\envs\\rlenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\pc\\.conda\\envs\\rlenv\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 30 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed9da19780a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mtd_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mnew_q_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mtd_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnew_q_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ed9da19780a1>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, s, a)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtd_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ed9da19780a1>\u001b[0m in \u001b[0;36mfeaturize\u001b[1;34m(self, s, a)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 30 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "## Code sample: SARSA with linear function approximation\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "\n",
    "class LinearEstimator:\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        self.w = np.zeros((20,20))\n",
    "        self.alpha = 0.05\n",
    "        \n",
    "    def featurize(self,s,a):\n",
    "        x =np.zeros(20)\n",
    "        x[s] = 1\n",
    "        x[16+a] =1\n",
    "        return x\n",
    "        \n",
    "    def predict(self,s,a):\n",
    "        return np.matmul(self.featurize(s,a),self.w)\n",
    "    \n",
    "    def update(self,s,a, td_target):\n",
    "        error = td_target-self.predict(s,a)\n",
    "        grad = self.featurize(s,a)\n",
    "        self.w += self.alpha*error*grad\n",
    "        \n",
    "    \n",
    "def epsilon_greedy_policy(estimator, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax([estimator.predict(state, a) for a in range(4)])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "estimator = LinearEstimator()\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "gamma = 1.\n",
    "\n",
    "n_episodes = 2000\n",
    "\n",
    "\n",
    "actions = range(env.action_space.n)\n",
    "# TO DO:\n",
    "score = []    \n",
    "for j in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    policy = epsilon_greedy_policy(estimator,epsilon=100./(j+1), actions = actions)\n",
    "    \n",
    "    ### Generate sample episode\n",
    "    while not done:\n",
    "        \n",
    "        action = policy(state)\n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "        \n",
    "        if reward < 1 and done:\n",
    "            reward = -1\n",
    "               \n",
    "        #Calculate the td_target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            new_q_val = estimator.predict(new_state, new_action)\n",
    "            td_target = reward + gamma * new_q_val\n",
    "        \n",
    "        estimator.update(state,action, td_target)    \n",
    "        \n",
    "        state, action = new_state, new_action\n",
    "            \n",
    "        if done:\n",
    "            if len(score) < 100:\n",
    "                score.append(reward)\n",
    "            else:\n",
    "                score[j % 100] = reward\n",
    "            print(\"\\rEpisode {} / {}. Avg score: {}\".format(j+1,n_episodes,np.mean(score)), end=\"\")\n",
    "            \n",
    "            break\n",
    "        if np.mean(score)>0.7:\n",
    "            sys.exit()\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
