{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Playing perfect defence with regret matching\n",
    "\n",
    "\n",
    "You have probably heard of reinforcement learning and how it is making headlines: [playing Atari](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/), [beating Go masters](https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught) and [saving energy for Google](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/), among other things.\n",
    "\n",
    "In a reinforcement learning problem there is an **agent** who receives a signal from the **environment** that encodes all the information it needs to take an **action** in order to maximize a certain **reward**. Examples of this situation are the ones above, but not only: choosing the content that is delievered to you online (ads or articles) is also an example, as well as adaptive treatments for chronical diseases. The framework is quite general, so it covers a number of applications.\n",
    "\n",
    "The aim is to find a way how to behave optimally against uncertainty: what actions should be done at each decision instant in order to maximize a reward.\n",
    "\n",
    "In this post, we will talk about reinforcement learning and game theory. Game theory is concerned with modelling strategic decision making for more than one agent. What does it have to do with reinforcement learning? It turns out that an agent can improve by playing against himself!. This idea is not new, but has turned out to be incredibly powerful: Recently, *Libratus*, an AI, won 1.7 million in chips (no real money) in Texas Hold-em. It played for 20 days, 11 hours per day, 4 of the top players. Why is this a thing? Unlike chess or go, poker is an incomplete information game, and such games were long believed to be too hard for computers. \n",
    "\n",
    "Around a month earlier than *Libratus*, a Czech team of researchers from Charles University and Czech Technical University created a similar AI, called *DeepStack*. You can read more about the comparison between them [here](http://www.nature.com/news/how-rival-bots-battled-their-way-to-poker-supremacy-1.21580). \n",
    "\n",
    "Formally, a two player zero sum game is a strategic interaction between two players, where one player's gain is at the expense of the other. Examples of zero sum games are:\n",
    "\n",
    "- Chess\n",
    "- Rock, paper, scissors\n",
    "- (Two person) Poker\n",
    "\n",
    "Note that these games are different in a few important ways. First, there's an issue with **information**: the state of the world is known to both players in chess, but not in poker, because both players are missing a crucial part of information (the hand of the opponent).\n",
    "\n",
    "We can summarize this as follows:\n",
    "\n",
    "- Chess: Perfect information, alternating and deterministic moves. \n",
    "- Rock, paper, scissors: Perfect information with simultaneous moves.\n",
    "- Poker: Incomplete information, alternating moves. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bit of game theory lingo: a **strategy** is a rule that tells each player how to behave, and a **strategy profile** is the set of strategies of the players. A strategy profile is a **Nash equilibrium (NE) ** if no player can do better by unilaterally changing his or her strategy. \n",
    "\n",
    "To see what this means, imagine that each player is told the strategies of the others. Suppose then that each player asks themselves: \"Knowing the strategies of the other players, and treating the strategies of the other players as set in stone, can I benefit by changing my strategy?\" If every player prefers not to switch (or is indifferent between switching and not) then the strategy profile is a Nash equilibrium. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, a Nash equilibrium teaches us to play self-defence: each player is doing the best it can. Since most interesting games have a certain component of luck, there is no guarantee to get a super-unbeatable strategy, so playing perfect self-defence is a desirable way.\n",
    "\n",
    "It is by no mean obvious to find Nash equilibria, however, for zero sum games, a way players can reach the Nash equilibrium strategy is through **self-play**. This roughly goes as follows:\n",
    "\n",
    "- Two agents start with a random strategy.\n",
    "- They improve their strategy after each game by some adaptive strategy.\n",
    "- When convergence is reached, your strategy is ready to go.\n",
    "\n",
    "The topic of this post is one of the simplest adaptive strategies, **regret matching**. It was introduced by Hart and Mas-Collel in 2000. The idea is that players reach equilibrium by tracking their regrets, and will play in the future those actions that led to higher regret. Let's illustrate this with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>Example</h4>\n",
    "<ul>\n",
    "    <li>Suppose we play rock, paper, scissors for money. Each of us puts 100 EUR on the table, we play and the winner takes the money. </li>\n",
    "    <li> The strategy sets are $I=J=\\{0,1,2\\}$ and the payoff is $u(i,j),$ which takes values between $-200, 200$. </li>\n",
    "    <li> You play rock and I play paper, so I win and take 100 EUR from you. Our payoffs are $(+100,-100)$. </li>\n",
    "    <li> Your <b>regret</b> for not playing paper is 100, but your regret for not paying scissors is even higher (200). </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we make an algorithm out of that? Simply play proportionally to your regret! The more you regret not doing a certain action, the more likely you are to play that action in the future.\n",
    "\n",
    "To put it clearly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Regret matching algorithm</h4>\n",
    "<ul>\n",
    "    <li> Initialize a counter for cumulative regret. </li>\n",
    "    <li> After each round, compute your regret for each action. If we played $(i^*,j^*)$, the regret for action $i$ is:\n",
    " $$\\max(u(i,j^*)-u(i^*,j^*),0).$$ </li>\n",
    "    <li> Add the regrets and normalize the sum (divide the regret of each strategy by the sum of regrets). </li>\n",
    "    <li> Play a mixed strategy that where each action is played proportionally to the cumulative regret. </li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regret matching is the basic ingredient of Libratus, the poker champion AI. It uses a selfplay algorithm (called CFR+) together with a special solver for the end of the game and a continual improvement meta-algorithm which improves after each match.\n",
    "\n",
    "[CFR](http://modelai.gettysburg.edu/2013/cfr/cfr.pdf) stands for *conterfactual regret minimization*. *Counterfactual* means \"If I had known\".This algorithm is roughly an adaptation of regret matching to the tree structure of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a simple example of regret minimization for rock, paper and scissors. \n",
    "First, we need to import a few standard libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regret minimization for rock, paper, scissors \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create our regret player as a class with two methods, one for calculating the regret and the other for calculating the mixed strategy corresponding to the regret vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegretPlayer():\n",
    "    def __init__(self):\n",
    "        ''' Keeps track of the history of the game'''\n",
    "        self.regret_sum = np.zeros(3)\n",
    "        self.my_moves = []\n",
    "        self.other_moves = []\n",
    "        \n",
    "    def move(self, strategy):\n",
    "        '''\n",
    "        Input: a vector of probabilities\n",
    "        Output: an action sampled from this vector\n",
    "        '''\n",
    "        return a\n",
    "\n",
    "    def regret(self):\n",
    "        '''Calculates the regret vector given the history'''\n",
    "        return regrets\n",
    "               \n",
    "    def get_regret_mixed_strategy(self):\n",
    "        '''Calculates the strategy from the regrets (normalizing the regret vector)'''\n",
    "        \n",
    "        return strategy            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to implement those yourself! Anyway, here goes the structure in a bit more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "class RegretPlayer():\n",
    "    def __init__(self):\n",
    "        self.regret_sum = np.zeros(3)\n",
    "        self.my_moves = []\n",
    "        self.other_moves = []\n",
    "        \n",
    "    def move(self, strategy):\n",
    "        # Input: a vector of probability distributions for actions\n",
    "        # Output: a pure action\n",
    "        \n",
    "        r = random.uniform(0,1)\n",
    "        n_actions = len(strategy)\n",
    "        \n",
    "        a = 0\n",
    "        cumulative_proba = 0.0\n",
    "        \n",
    "        while a<n_actions-1:\n",
    "            cumulative_proba += strategy[a] \n",
    "            if r < cumulative_proba: return a\n",
    "            a +=1\n",
    "        return a\n",
    "\n",
    "    def regret(self):\n",
    "        \n",
    "        if len(self.my_moves)>0:\n",
    "            my_action = self.my_moves[-1]\n",
    "            his_action = self.other_moves[-1]\n",
    "        else:\n",
    "            return np.zeros(3)\n",
    "        \n",
    "        # Payoffs from player's perspective perspective\n",
    "        my_payoff = np.zeros(3)\n",
    "        \n",
    "        # If we play the same, I don't get any payoff\n",
    "        my_payoff[his_action] = 0.\n",
    "                 \n",
    "        # I win when he plays scissors and I pay rock, \n",
    "        # or when I play the \"next\" (Rock = 0, Paper = 1, Scissors = 2)\n",
    "        my_payoff[0 if his_action == 2 else his_action + 1] = 1\n",
    "                \n",
    "        # I lose when I play scissors and he plays rock, \n",
    "        # or when I play the \"previous\" action         \n",
    "        my_payoff[2 if his_action == 0 else his_action -1] = -1\n",
    "                 \n",
    "        regrets = [my_payoff[a]-my_payoff[my_action] for a in range(3)]\n",
    "        regrets = np.array(regrets)\n",
    "        return regrets\n",
    "               \n",
    "    def get_regret_mixed_strategy(self):\n",
    "    \n",
    "        normalize_sum = 0.0\n",
    "        strategy = np.zeros(3)\n",
    "        regret = self.regret()                   \n",
    "        \n",
    "        for a in range(3):\n",
    "            strategy[a] = max(self.regret_sum[a],0)\n",
    "            normalize_sum += strategy[a]\n",
    "            \n",
    "        # If all regrets are positive, play randomly\n",
    "        if normalize_sum > 0:\n",
    "            strategy = strategy / normalize_sum\n",
    "        else:\n",
    "            strategy = np.ones(3)/3\n",
    "        \n",
    "        self.regret_sum += regret\n",
    "        \n",
    "        return strategy            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function that simulates the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_rounds = 1000, verbose=False):\n",
    "    p1 = RegretPlayer()\n",
    "    p2 = RegretPlayer() \n",
    "    strategies = []\n",
    "    total_p1 = 0.0\n",
    "    total_p2 = 0.0\n",
    "\n",
    "    for n in range(1,n_rounds):\n",
    "        \n",
    "        regret_strategy_p1 = p1.get_regret_mixed_strategy()\n",
    "        regret_strategy_p2 = p2.get_regret_mixed_strategy()\n",
    "        \n",
    "        m1 = p1.move(regret_strategy_p1)      \n",
    "        m2 = p2.move(regret_strategy_p2)\n",
    "        \n",
    "        # Save the regret strategies\n",
    "        strategies.append((regret_strategy_p1,regret_strategy_p2))\n",
    "                \n",
    "        # Players update the info of the moves\n",
    "        p1.my_moves.append(m1)\n",
    "        p1.other_moves.append(m2)\n",
    "        \n",
    "        p2.my_moves.append(m2)\n",
    "        p2.other_moves.append(m1)    \n",
    "            \n",
    "\n",
    "        #### Display results: useful for debugging\n",
    "        if verbose:\n",
    "            moves_map = {0:\"Rock\", 1:\"Paper\", 2:\"Scissors\"}\n",
    "            print('-'*50)\n",
    "            print(\"Player 1 strategy:\", regret_strategy_p1)\n",
    "            print(\"Player 2 strategy:\", regret_strategy_p2)\n",
    "            print(\"My move: %s\" % moves_map[m1])\n",
    "            print(\"His move: %s\" % moves_map[m2])\n",
    "\n",
    "\n",
    "    return strategies\n",
    "\n",
    "eq = run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is the **average** strategy profile that converges to the Nash equilibrium strategy! In this case, we know that the Nash equilibrium strategy is to play randomly each action with equal probability, that means, we expect the vector $\\left (\\frac{1}{3},\\frac{1}{3},\\frac{1}{3} \\right).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average strategy of player 1:  [ 0.32  0.31  0.37]\n",
      "Average strategy of player 2:  [ 0.34  0.34  0.32]\n"
     ]
    }
   ],
   "source": [
    "average_strategy_p1 = np.mean([x[0] for x in eq], axis=0)\n",
    "average_strategy_p2 = np.mean([x[1] for x in eq], axis=0)\n",
    "print(\"Average strategy of player 1: \", average_strategy_p1)\n",
    "print(\"Average strategy of player 2: \", average_strategy_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Nash equilibrium strategy ready to use! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
