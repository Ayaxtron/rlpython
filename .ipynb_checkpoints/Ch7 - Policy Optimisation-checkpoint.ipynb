{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients & Policy Optimisation\n",
    "\n",
    "\n",
    "Up to now, we have been focused in **value-Based RL**. This means, we learn the value function, and use the policy that is implicitly obtained from it. What we will do now is to turn it around, and go for **policy-based RL**: we just learn directly the policy, without the value function. A intermediate class of methods is called **Actor-Critic**, where both policy and value function are learnt. This later approach will not be discussed here.\n",
    "\n",
    "Why would this make sense? Well, sometimes we may just want to know what to do (left/right), not how bad or good it is to do either.\n",
    "\n",
    "So far, we have obtained policies through the $Q$-function (e.g. using $\\epsilon$-greedy), but we can parameterise directly the **policy**:\n",
    "\t$$ \\pi_\\theta(s,a) = \\mathbb P(a \\ | \\ s, \\theta) $$\n",
    "As with the value functions, if we use a differentiable parametrisation of the policy, we can update it by gradient descent.\n",
    "\n",
    "\n",
    "For example, we can assume the policy has the following structure:\n",
    "$$\\pi_\\theta(s,a) = \\frac{\\mathrm{exp}(h(s,a,\\theta))}{\\sum_{a'}\\mathrm{exp}(h(s,a',\\theta))}$$\n",
    "\n",
    "where $h(s,a,\\theta) = \\theta^T\\cdot\\mathbf{x}(s,a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The advantages of policy gradient are:\n",
    "- **Stronger convergence guarantees** (at least to a local optimum). \n",
    "- Effective in continuous action spaces.\n",
    "- No maximization of the $Q$ function is needed, which also means that the update is less \"aggressive\" than in $Q$-learning.\n",
    "\n",
    "\n",
    "\n",
    "### Objective function\n",
    "- **Goal**: find the best parameter $\\theta$ for $\\pi_\\theta(s,a)$, according a performance functional $J(\\theta)$.\n",
    "- Update according to the rule:\n",
    "$$ \\theta_{t+1} = \\theta_t + \\nabla \\hat{J}(\\theta_t)$$\n",
    "where $\\hat{J}$ is a stochastic approximation of the gradient (obtained, for example, by gradient **ascent**.\n",
    "\n",
    "\n",
    "## Derivative Free Methods\n",
    "\n",
    "\n",
    "\n",
    "Why bother with gradients? There is a large class of optimization methods that do not use of the gradients (the expensive part of the computation), and are easy to parallelize. We will discuss two methods which have shown promising results.\n",
    "\n",
    "\n",
    "### Cross-Entropy Method\n",
    "\n",
    "* Blackbox optimization for $f(w)$.\n",
    "* Start with $\\mu, \\sigma$, `batch_size`, `elite_frac`.\n",
    "* Do forever:\n",
    "  * Sample `batch_size` possible values for $w$.\n",
    "  * Evaluate $f$ on the sampled values.\n",
    "  * Keep the top `elite_frac` of those. These are `elite_set`.\n",
    "  * $\\mu \\leftarrow$ `np.mean(elite_set)`\n",
    "  * $\\sigma \\leftarrow$ `np.std(elite_set)`\n",
    "\n",
    "\n",
    "### Natural Evolution Strategies\n",
    "\n",
    "- https://blog.openai.com/evolution-strategies/\n",
    "- Comparable results to A3C (state of the art (?)).\n",
    "\n",
    "\n",
    "\n",
    "### NES\n",
    "* Start with $\\sigma$, $\\alpha$, `n_estimators`.\n",
    "* Sample $\\epsilon_1, \\epsilon_2$ from a normal distribution with mean $0$ and variance $1$. These are **vectors**.\n",
    "* Calculate return $f(w+\\sigma \\cdot \\epsilon_i)$.\n",
    "*  $w \\leftarrow w + \\alpha\\cdot \\frac{1}{N}\\sum_{i=1}^n\\left(\\frac{f(w+\\sigma\\epsilon_i)-f(w)}{\\sigma}\\epsilon_i\\right)$ \n",
    "* Stopping criteria: function changes below a threshold, or, in the case of OpenAI Gym, performance benchmark is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9226c1a98061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mw_try\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# jitter jw using gaussian of sigma 0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_try\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# evaluate the jittered version\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[1;31m# standardize the rewards to have a gaussian distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9226c1a98061>\u001b[0m in \u001b[0;36mf\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mtotal_reward\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Exercise\n",
    "\n",
    "# Implement NES and CEM for a generic function\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "### the function we want to optimize\n",
    "def f(w):\n",
    "    center = np.array([0.5, 0.1, -0.3])  \n",
    "    reward = -np.sum(np.square(center - w))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def nes(npop, n_iter, sigma, alpha, f, w0):\n",
    "###YOUR IMPLEMENTATION\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5. w: [ 0.09757403  0.44087563  0.19807915  0.05933475], reward: 32.995000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dcae72c08f5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mw_try\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# jitter jw using gaussian of sigma 0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_try\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# evaluate the jittered version\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m   \u001b[1;31m# standardize the rewards to have a gaussian distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-dcae72c08f5f>\u001b[0m in \u001b[0;36mf\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\rlenv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\rlenv\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\rlenv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\rlenv\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Code sample: NES for FrozenLake\n",
    "import numpy as np\n",
    "import gym\n",
    "np.random.seed(0)\n",
    "\n",
    "env= gym.make('CartPole-v0')\n",
    "\n",
    "# the function we want to optimize\n",
    "def f(w):\n",
    "    n_iter = 200\n",
    "    total_reward = 0\n",
    "    for it in range(n_iter):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = 1 if np.matmul(state,w) < 0 else 0\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            total_reward+=reward\n",
    "            state = new_state\n",
    "    return total_reward/(1+it)\n",
    "\n",
    "# hyperparameters\n",
    "npop = 100 # population size\n",
    "sigma = 0.1 # noise standard deviation\n",
    "alpha = 0.001 # learning rate\n",
    "\n",
    "w = 2*np.random.random(4)-1 # our initial guess is random\n",
    "\n",
    "i = 0\n",
    "\n",
    "while f(w) <200:\n",
    "    i+=1\n",
    "  # print current fitness of the most likely parameter setting\n",
    "    if i % 5 == 0:\n",
    "        print('iter %d. w: %s, reward: %f' %(i, str(w),  f(w)))\n",
    "\n",
    "  # initialize memory for a population of w's, and their rewards\n",
    "    N = np.random.randn(npop, 4) # samples from a normal distribution N(0,1)\n",
    "    R = np.zeros(npop)\n",
    "    for j in range(npop):\n",
    "        w_try = w + sigma*N[j] # jitter jw using gaussian of sigma 0.1\n",
    "        R[j] = f(w_try) # evaluate the jittered version\n",
    "    \n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    A = (R - np.mean(R)) / np.std(R)\n",
    "    # perform the parameter update. The matrix multiply below\n",
    "    # is just an efficient way to sum up all the rows of the noise matrix N,\n",
    "    # where each row N[j] is weighted by A[j]\n",
    "    w = w + alpha/(npop*sigma+1) * np.dot(N.T, A)\n",
    "  \n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code sample: Cross entropy method for CartPole\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Task settings:\n",
    "env = gym.make('CartPole-v0') # Change as needed\n",
    "num_steps = 500 # maximum length of episode\n",
    "# Alg settings:\n",
    "n_iter = 100 # number of iterations of CEM\n",
    "batch_size = 25 # number of samples per batch\n",
    "elite_frac = 0.2 # fraction of samples used as elite set\n",
    "\n",
    "\n",
    "# Initialize mean and standard deviation\n",
    "dim_theta = (env.observation_space.shape[0]+1) * env.action_space.n\n",
    "theta_mean = np.zeros(dim_theta)\n",
    "theta_std = np.ones(dim_theta)\n",
    "\n",
    "\n",
    "def make_policy(theta):\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    W = theta[0 : n_states * n_actions].reshape(n_states, n_actions)\n",
    "    b = theta[n_states * n_actions : None].reshape(1, n_actions)\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        y = state.dot(W) + b\n",
    "        action = y.argmax()\n",
    "        return action\n",
    "        \n",
    "    return policy_fn\n",
    "\n",
    "def run_episode(theta, num_steps, render=False):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    policy = make_policy(theta)\n",
    "    for t in range(num_steps):\n",
    "        a = policy(state)\n",
    "        state, reward, done, _ = env.step(a)\n",
    "        total_reward += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_reward\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    total_reward = run_episode(theta, num_steps)\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Now, for the algorithms\n",
    "for it in range(n_iter):\n",
    "    # Sample parameter vectors\n",
    "    thetas = np.random.normal(theta_mean, theta_std, (batch_size,dim_theta))\n",
    "    rewards = [noisy_evaluation(theta) for theta in thetas]\n",
    "    # Get elite parameters\n",
    "    n_elite = int(batch_size * elite_frac)\n",
    "    elite_inds = np.argsort(rewards)[batch_size - n_elite:batch_size]\n",
    "    elite_thetas = [thetas[i] for i in elite_inds]\n",
    "    # Update theta_mean, theta_std\n",
    "    theta_mean = np.mean(elite_thetas,axis=0)\n",
    "    theta_std = np.std(elite_thetas,axis=0)\n",
    "    print(\"Mean reward f: {}. Max reward: {}\".format(np.mean(rewards), np.max(rewards)))\n",
    "    run_episode(theta_mean, num_steps, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
